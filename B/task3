import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score

# ------------------------------
# Utility Functions
# ------------------------------
def encode_one_hot(labels, num_classes=10):
    encoded = np.zeros((len(labels), num_classes))
    encoded[np.arange(len(labels)), labels] = 1
    return encoded

def apply_softmax(logits):
    if logits.ndim == 2:
        exp_vals = np.exp(logits - np.max(logits, axis=1, keepdims=True))
        return exp_vals / (np.sum(exp_vals, axis=1, keepdims=True) + 1e-8)
    else:
        exp_vals = np.exp(logits - np.max(logits))
        return exp_vals / (np.sum(exp_vals) + 1e-8)

def compute_cross_entropy(predictions, targets, epsilon=1e-8):
    num_samples = predictions.shape[0]
    loss = -np.sum(targets * np.log(predictions + epsilon)) / num_samples
    return loss

# ------------------------------
# Load and Preprocess Dataset
# ------------------------------
print("Fetching MNIST dataset...")
mnist_data = fetch_openml('mnist_784', version=1, as_frame=False)
features = mnist_data.data.astype(np.float32) / 255.0  # Normalize pixel values
labels = mnist_data.target.astype(np.int64)
features_reshaped = features.reshape(-1, 28, 28)      # Reshape for CNN compatibility
one_hot_labels = encode_one_hot(labels, 10)

# Split dataset into training and testing sets
X_train_mlp, X_test_mlp, y_train, y_test, y_train_one_hot, y_test_one_hot = train_test_split(
    features, labels, one_hot_labels, test_size=0.2, random_state=42
)
X_train_cnn, X_test_cnn, _, _, _, _ = train_test_split(
    features_reshaped, labels, one_hot_labels, test_size=0.2, random_state=42
)

print("Dataset loaded and processed.")

# ------------------------------
# Define Multi-Layer Perceptron (MLP)
# ------------------------------
class MLPNetwork:
    def __init__(self, input_size, hidden_layers, output_size):
        self.weights = []
        self.biases = []
        layer_structure = [input_size] + hidden_layers + [output_size]
        
        for i in range(len(layer_structure) - 1):
            self.weights.append(np.random.randn(layer_structure[i], layer_structure[i+1]) * np.sqrt(2. / layer_structure[i]))
            self.biases.append(np.zeros((1, layer_structure[i+1])))
    
    def forward_pass(self, inputs):
        self.activations = {}
        output = inputs
        self.activations['A0'] = output
        num_layers = len(self.weights)
        
        for i in range(num_layers - 1):
            z = np.dot(output, self.weights[i]) + self.biases[i]
            self.activations[f'Z{i+1}'] = z
            output = np.maximum(0, z)  # ReLU activation
            self.activations[f'A{i+1}'] = output
        
        final_z = np.dot(output, self.weights[-1]) + self.biases[-1]
        self.activations[f'Z{num_layers}'] = final_z
        final_output = apply_softmax(final_z)
        self.activations[f'A{num_layers}'] = final_output
        return final_output
    
    def backward_pass(self, inputs, targets, learning_rate=0.01):
        num_samples = inputs.shape[0]
        num_layers = len(self.weights)
        error_signal = self.activations[f'A{num_layers}'] - targets  # Softmax derivative
        
        for layer in reversed(range(num_layers)):
            prev_activation = self.activations[f'A{layer}']
            weight_gradient = np.dot(prev_activation.T, error_signal) / num_samples
            bias_gradient = np.sum(error_signal, axis=0, keepdims=True) / num_samples
            
            self.weights[layer] -= learning_rate * weight_gradient
            self.biases[layer] -= learning_rate * bias_gradient
            
            if layer > 0:
                prev_z = self.activations[f'Z{layer}']
                prev_activation_error = np.dot(error_signal, self.weights[layer].T)
                error_signal = prev_activation_error * (prev_z > 0)  # ReLU derivative

# ------------------------------
# Train MLP Model
# ------------------------------
epochs_mlp = 20
learning_rate_mlp = 0.01
batch_size = 128

print("Training MLP Model...")
mlp_model = MLPNetwork(input_size=784, hidden_layers=[128, 64], output_size=10)
training_losses = []
testing_losses = []
num_training_samples = X_train_mlp.shape[0]

for epoch in range(epochs_mlp):
    shuffled_indices = np.arange(num_training_samples)
    np.random.shuffle(shuffled_indices)
    X_train_shuffled = X_train_mlp[shuffled_indices]
    y_train_shuffled = y_train_one_hot[shuffled_indices]
    
    epoch_loss = 0.0
    num_batches = int(np.ceil(num_training_samples / batch_size))
    
    for batch in range(num_batches):
        start = batch * batch_size
        end = start + batch_size
        X_batch = X_train_shuffled[start:end]
        y_batch = y_train_shuffled[start:end]
        
        y_predicted = mlp_model.forward_pass(X_batch)
        batch_loss = compute_cross_entropy(y_predicted, y_batch)
        epoch_loss += batch_loss
        mlp_model.backward_pass(X_batch, y_batch, learning_rate=learning_rate_mlp)
    
    epoch_loss /= num_batches
    training_losses.append(epoch_loss)
    
    y_pred_test = mlp_model.forward_pass(X_test_mlp)
    test_loss = compute_cross_entropy(y_pred_test, y_test_one_hot)
    testing_losses.append(test_loss)
    
    training_accuracy = accuracy_score(y_train, np.argmax(mlp_model.forward_pass(X_train_mlp), axis=1))
    testing_accuracy = accuracy_score(y_test, np.argmax(y_pred_test, axis=1))
    print(f"Epoch {epoch+1}/{epochs_mlp} - Train Loss: {epoch_loss:.4f} | Test Loss: {test_loss:.4f} | Train Acc: {training_accuracy:.4f} | Test Acc: {testing_accuracy:.4f}")

# Plot Results
plt.figure(figsize=(6, 5))
mlp_predictions = np.argmax(mlp_model.forward_pass(X_test_mlp), axis=1)
confusion_mtx = confusion_matrix(y_test, mlp_predictions)
sns.heatmap(confusion_mtx, annot=True, fmt="d", cmap="Blues")
plt.title("MLP Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.savefig('plots/mlp_confusion_matrix.png')
plt.show()

plt.figure()
plt.plot(range(1, epochs_mlp + 1), training_losses, label="Training Loss")
plt.plot(range(1, epochs_mlp + 1), testing_losses, label="Testing Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("MLP Training Progress")
plt.legend()
plt.savefig('plots/mlp_training_progress.png')
plt.show()
